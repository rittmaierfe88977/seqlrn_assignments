{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 2: POTUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1) President of the United States (Trump vs. Obama)\n",
    "\n",
    "Surely, you're aware that the 45th President of the United States (@POTUS45) was an active user of Twitter, until (permanently) banned on Jan 8, 2021.\n",
    "You can still enjoy his greatness at the [Trump Twitter Archive](https://www.thetrumparchive.com/). We will be using original tweets only, so make sure to remove all retweets.\n",
    "Another fan of Twitter was Barack Obama (@POTUS43 and @POTUS44), who used the platform in a rather professional way.\n",
    "Please also consider the POTUS Tweets of Joe Biden; we will be using those for testing.\n",
    "\n",
    "### Data\n",
    "\n",
    "There are multiple ways to get the data, but the easiest way is to download the files from the `Supplemental Materials` in the `Files` section of our Microsoft Teams group. \n",
    "Another way is to directly use the data from [Trump Twitter Archive](https://www.thetrumparchive.com/), [Obama Kaggle](https://www.kaggle.com/jayrav13/obama-white-house), and [Biden Kaggle](https://www.kaggle.com/rohanrao/joe-biden-tweets).\n",
    "Before you get started, please download the files; you can put them into the data folder.\n",
    "\n",
    "### N-gram Models\n",
    "\n",
    "In this assignment, you will be doing some Twitter-related preprocessing and training n-gram models to be able to distinguish between Tweets of Trump, Obama, and Biden.\n",
    "We will be using [NLTK](https://www.nltk.org), more specifically it's [`lm`](https://www.nltk.org/api/nltk.lm.html) module. \n",
    "Install the NLTK package within your working environment.\n",
    "You can use some of the NLTK functions, but you have to implement the functions for likelihoods and perplexity from scratch.\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 Prepare all the Tweets. Since the `lm` modules will work on tokenized data, implement a tokenization method that strips unnecessary tokens but retains special words such as mentions (@...) and hashtags (#...).\n",
    "\n",
    "1.2 Partition into training and test sets; select about 100 tweets each, which we will be testing on later. As with any Machine Learning task, training and test must not overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "c2ae5b5d-fccd-4092-af20-d8e8b4a65ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice: ignore retweets \n",
    "def isRetweet(text: str):\n",
    "    return text.startswith(\"RT @\")\n",
    "\n",
    "\n",
    "def load_trump_tweets(filepath):\n",
    "    \"\"\"Loads all Trump tweets and returns them as a list.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    tweets = []\n",
    "    df = pd.read_csv(filepath, sep=\",\")\n",
    "    \n",
    "    for _, data in df.iterrows():\n",
    "        if data[\"isRetweet\"] == \"f\":\n",
    "            tweets.append(data[\"text\"])\n",
    "\n",
    "    return tweets\n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def load_obama_tweets(filepath):\n",
    "    \"\"\"Loads all Obama tweets and returns them as a list.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    tweets = []\n",
    "    df = pd.read_csv(filepath, sep=\",\")\n",
    "    \n",
    "    for _, data in df.iterrows():\n",
    "        if not isRetweet(data[\"Tweet-text\"]):\n",
    "            tweets.append(data[\"Tweet-text\"])\n",
    "\n",
    "    return tweets\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "\n",
    "def load_biden_tweets(filepath):\n",
    "    \"\"\"Loads all Biden tweets and returns them as a list.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    tweets = []\n",
    "    df = pd.read_csv(filepath, sep=\",\")\n",
    "    for _, data in df.iterrows():\n",
    "        if not isRetweet(data[\"tweet\"]):\n",
    "            tweets.append(data[\"tweet\"])\n",
    "\n",
    "    return tweets\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "f25e8c56-3837-440b-bebe-1916ebede6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice: think about start and end tokens\n",
    "\n",
    "NUM_TEST = 100\n",
    "TOKENS_PATTERN = r\"\"\"(\n",
    "    (?:@[\\w_]+)                # @mention\n",
    "  | (?:\\#[\\w_]+)               # #hashtag\n",
    "  | (?:https?://\\S+)          # URL\n",
    "  | (?:[A-Za-z0-9_]+(?:'[A-Za-z0-9_]+)?)  # words with optional apostrophes\n",
    "  | (?:[.,!?;])               # punctuation\n",
    "  | (?:\\S)                    # catch-all for emojis/symbols/etc.\n",
    ")\"\"\"\n",
    "\n",
    "def tokenize(text, pattern=TOKENS_PATTERN):\n",
    "    \"\"\"Tokenizes a single Tweet.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    return [\"<s>\"] + re.findall(pattern, text, re.VERBOSE) + [\"</s>\"]\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "\n",
    "def split_and_tokenize(data, num_test=NUM_TEST):\n",
    "    \"\"\"Splits and tokenizes the given list of Twitter tweets.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    tokenized_tweets = []\n",
    "    num_tweets = num_test if num_test > 0 else len(data)\n",
    "    for i in range(num_tweets):\n",
    "        tokenized_tweets.append(tokenize(data[i]))\n",
    "\n",
    "    return tokenized_tweets\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "2598cfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Republicans', 'and', 'Democrats', 'have', 'both', 'created', 'our', 'economic', 'problems', '.', '</s>']\n",
      "['<s>', 'From', 'a', 'big', 'NBA', 'fan', 'congrats', 'to', 'future', 'Hall', 'of', 'Famers', 'Dwyane', 'Wade', 'and', 'Dirk', 'Nowitzki', 'â€”', 'not', 'just', 'all', '-', 'time', 'greats', 'but', 'class', 'acts', 'too', '.', '</s>']\n",
      "['<s>', 'Tune', 'in', '11', ':', '30', 'ET', 'tomorrow', 'for', 'a', 'live', 'webcast', 'of', 'Families', 'USA', 'Presidential', 'Forum', 'on', 'health', 'care', ':', 'http://presidentialforums.health08.org/', '</s>']\n"
     ]
    }
   ],
   "source": [
    "trump_tweets = split_and_tokenize(load_trump_tweets(\"C:\\\\Users\\\\Felix\\\\PythonProjects\\\\seqlrn_assignments\\\\2-markov-chains\\\\data\\\\tweets\\\\tweets_trump.csv\"))\n",
    "obama_tweets = split_and_tokenize(load_obama_tweets(\"C:\\\\Users\\\\Felix\\\\PythonProjects\\\\seqlrn_assignments\\\\2-markov-chains\\\\data\\\\tweets\\\\tweets_obama.csv\"))\n",
    "biden_tweets = split_and_tokenize(load_biden_tweets(\"C:\\\\Users\\\\Felix\\\\PythonProjects\\\\seqlrn_assignments\\\\2-markov-chains\\\\data\\\\tweets\\\\tweets_biden.csv\"))\n",
    "\n",
    "print(trump_tweets[0])\n",
    "print(obama_tweets[0])\n",
    "print(biden_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c32c33-1a19-42ac-93d0-fcf66fbeaf5f",
   "metadata": {},
   "source": [
    "### Train N-gram Models\n",
    "\n",
    "2.1 Train n-gram models with n = [1, ..., 5] for Obama, Trump, and Biden.\n",
    "\n",
    "2.2 Also train a joint model, that will serve as background model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "0828bda0-cef2-428b-a238-7f07f2c25425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_n_gram_models(n, data):\n",
    "    \"\"\"\n",
    "    To predict the first few words of the Tweet, we need the smaller n-grams as\n",
    "    well. This method does calculate all n-grams up to the given n.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    n_gram_models = []\n",
    "\n",
    "    for i in range(1, n+1):\n",
    "        i_gram = []\n",
    "\n",
    "        for tweet in data:\n",
    "            i_gram.extend(nltk.ngrams(sequence=tweet, n=i))\n",
    "        \n",
    "        n_gram_models.append(i_gram)\n",
    "\n",
    "    return n_gram_models\n",
    "    ### END YOUR CODE\n",
    "\n",
    "def get_suggestion(prev, n_gram_model):\n",
    "    \"\"\"\n",
    "    Gets the next random word for the given n_grams.\n",
    "    The size of the previous tokens must be exactly one less than the n-value\n",
    "    of the n-gram, or it will not be able to make a prediction.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    n = len(n_gram_model[0])\n",
    "             \n",
    "    if len(prev) > n - 1:\n",
    "        raise ValueError(f\"{n}-gram needs {n} tokens for prediction.\")\n",
    "\n",
    "    ngram_counts = Counter(n_gram_model)\n",
    "    all_sequences_count = sum(ngram_counts.values())\n",
    "    candidates = [(sequence, count / all_sequences_count) for sequence, count in ngram_counts.items() \n",
    "                  if prev == list(sequence[:-1])]\n",
    "    \n",
    "    if len(candidates) < 1:\n",
    "        random_suggestion = random.choice(list(ngram_counts.keys()))\n",
    "        print(\"No candidate found, returning random token!\")\n",
    "        return random_suggestion[-1]\n",
    "\n",
    "    candidates.sort(key=lambda c: c[1], reverse=True)\n",
    "\n",
    "    return candidates[0][0][-1]  \n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def get_random_tweet(n, n_gram_models):\n",
    "    \"\"\"Generates a random tweet using the given data set.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    tweet_sequence = [\"Make\", \"america\"]\n",
    "\n",
    "    for i in range(len(tweet_sequence), n+1):\n",
    "        context = tweet_sequence[-(i-1):]\n",
    "        suggestion = get_suggestion(context, n_gram_models[n-1])\n",
    "        tweet_sequence.append(suggestion)\n",
    "        if i == n:\n",
    "            while suggestion != \"</s>\" and len(tweet_sequence) < 30:\n",
    "                context = tweet_sequence[-(i-1):]\n",
    "                suggestion = get_suggestion(context, n_gram_models[n-1]) \n",
    "                tweet_sequence.append(suggestion)\n",
    "\n",
    "    return \" \".join([str(w) for w in tweet_sequence if w not in (\"<s>\", \"</s>\")])    \n",
    "\n",
    "\n",
    "def get_random_tweet(n, n_gram_models):\n",
    "    \"\"\"Generates a random tweet using the given n-gram model.\"\"\"\n",
    "    tweet_sequence = [\"<s>\", \"Make\", \"America\"]\n",
    "\n",
    "    while True:\n",
    "        # Kontext: Letzte (n-1) WÃ¶rter\n",
    "        context = tweet_sequence[-(n-1):] if n > 1 else []\n",
    "\n",
    "        suggestion = get_suggestion(context, n_gram_models[n-1])\n",
    "        tweet_sequence.append(suggestion)\n",
    "\n",
    "        if suggestion == \"</s>\" or len(tweet_sequence) >= 30:\n",
    "            break\n",
    "\n",
    "    # RÃ¼ckgabe ohne Start/End-Token\n",
    "    return \" \".join([w for w in tweet_sequence if w not in (\"<s>\", \"</s>\")])\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "37e5cc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No candidate found, returning random token!\n",
      "No candidate found, returning random token!\n",
      "No candidate found, returning random token!\n",
      "No candidate found, returning random token!\n",
      "No candidate found, returning random token!\n",
      "No candidate found, returning random token!\n",
      "No candidate found, returning random token!\n",
      "No candidate found, returning random token!\n",
      "No candidate found, returning random token!\n",
      "No candidate found, returning random token!\n",
      "No candidate found, returning random token!\n",
      "No candidate found, returning random token!\n",
      "No candidate found, returning random token!\n",
      "No candidate found, returning random token!\n",
      "No candidate found, returning random token!\n",
      "No candidate found, returning random token!\n",
      "No candidate found, returning random token!\n",
      "No candidate found, returning random token!\n",
      "No candidate found, returning random token!\n",
      "Make America have for No peace turning 2 inaccurate @US_FDA the hotly values @SecretarySonny points the Left from than they â€™ ll be back in the Great city of Charlotte\n"
     ]
    }
   ],
   "source": [
    "n_gram_models = build_n_gram_models(n=3, data=trump_tweets)\n",
    "random_tweet_trump = get_random_tweet(n=3, n_gram_models=n_gram_models)\n",
    "print(random_tweet_trump)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648216bb-a49e-45ff-bb8c-9094c33acc07",
   "metadata": {},
   "source": [
    "### Classify the Tweets\n",
    "\n",
    "3.1 Use the log-ratio method to classify the Tweets for Trump vs. Biden. Trump should be easy to spot; but what about Obama vs. Biden?\n",
    "\n",
    "3.2 Analyze: At what context length (n) does the system perform best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "99dd4ca5-8094-40b0-aa1b-a51268659397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_single_token_log_ratio(prev, token, n_gram_model1, n_gram_model2):\n",
    "    \"\"\"Calculates the log ration of a token for two different n-grams\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def classify(n, tokens, n_gram_models1, n_gram_models2):\n",
    "    \"\"\"\n",
    "    Checks which of the two given datasets is more likely for the given Tweet.\n",
    "    If true is returned, the first one is more likely, otherwise the second.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    ### END YOUR CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97d1e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(n, data1, data2, classify_fn):\n",
    "    \"\"\"\n",
    "    Trains the n-gram models on the train data and validates on the test data.\n",
    "    Uses the implemented classification function to predict the Tweeter.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f5f88a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_length = ...\n",
    "# validate(context_length, trump_tweets, biden_tweets, classify_fn=classify)\n",
    "# validate(context_length, obama_tweets, biden_tweets, classify_fn=classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298e1fb7-c67b-4e44-87c7-488e704c5ac1",
   "metadata": {},
   "source": [
    "### Compute Perplexities\n",
    "\n",
    "4.1 Compute (and plot) the perplexities for each of the test tweets and models. Is picking the Model with minimum perplexity a better classifier than in 3.1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9bfe2154-d816-442e-8de8-3b836ab0ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_perplexity(n, tokens, n_gram_models1, n_gram_models2):\n",
    "    \"\"\"\n",
    "    Checks which of the two given datasets is more likely for the given Tweet.\n",
    "    If true is returned, the first one is more likely, otherwise the second.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b3be177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_length = ...\n",
    "# validate(context_length, trump_tweets, biden_tweets, classify_fn=classify_with_perplexity)\n",
    "# validate(context_length, obama_tweets, biden_tweets, classify_fn=classify_with_perplexity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seqlrn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
