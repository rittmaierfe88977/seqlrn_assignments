{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 4: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1) Skip-grams\n",
    "\n",
    "Tomas Mikolov's [original paper](https://arxiv.org/abs/1301.3781) for word2vec is not very specific on how to actually compute the embedding matrices.\n",
    "Xin Ron provides a much more detailed [walk-through](https://arxiv.org/pdf/1411.2738.pdf) of the math, I recommend you go through it before you continue with this assignment.\n",
    "Now, while the original implementation was in C and estimates the matrices directly, in this assignment, we want to use PyTorch (and autograd) to train the matrices.\n",
    "There are plenty of example implementations and blog posts out there that show how to do it, I particularly recommend [Mateusz Bednarski's](https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb) version. Familiarize yourself with skip-grams and how to train them using pytorch.\n",
    "\n",
    "### Data\n",
    "\n",
    "Download the `theses.csv` data set from the `Supplemental Materials` in the `Files` section of our Microsoft Teams group.\n",
    "This dataset consists of approx. 3,000 theses topics chosen by students in the past.\n",
    "Here are some examples of the file content:\n",
    "\n",
    "```\n",
    "27.10.94;14.07.95;1995;intern;Diplom;DE;Monte Carlo-Simulation für ein gekoppeltes Round-Robin-System;\n",
    "04.11.94;14.03.95;1995;intern;Diplom;DE;Implementierung eines Testüberdeckungsgrad-Analysators für RAS;\n",
    "01.11.20;01.04.21;2021;intern;Bachelor;DE;Landessprachenerkennung mittels X-Vektoren und Meta-Klassifikation;\n",
    "```\n",
    "\n",
    "### Basic Setup\n",
    "\n",
    "For the upcoming assignments on Neural Networks, we'll be heavily using [PyTorch](https://pytorch.org) as go-to Deep Learning library.\n",
    "If you're not already familiar with PyTorch, now's the time to get started with it.\n",
    "Head over to the [Basics](https://pytorch.org/tutorials/beginner/basics/intro.html) and gain some understanding about the essentials.\n",
    "Before starting this assignment, make sure you've got PyTorch installed in your working environment. \n",
    "It's a quick setup, and you'll find all the instructions you need on the PyTorch website.\n",
    "As always, you can use [NumPy](https://numpy.org) and [Pandas](https://pandas.pydata.org) for data handling etc.\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 Spend some time on preparing the dataset. It may be helpful to lower-case the data and to filter for German titles. The format of the CSV-file should be:\n",
    "\n",
    "```\n",
    "Anmeldedatum;Abgabedatum;JahrAkademisch;Art;Grad;Sprache;Titel;Abstract\n",
    "```\n",
    "\n",
    "1.2 Create the vocabulary from the prepared dataset. You'll need it for the initialization of the matrices and to map tokens to indices.\n",
    "\n",
    "1.3 Generate the training pairs with center word and context word. Which window size do you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c9e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_theses_dataset(filepath):\n",
    "    \"\"\"Loads all theses instances and returns them as a dataframe.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    return pd.read_csv(filepath, sep=\"\\t\", header=0, encoding='ANSI')\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa699c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataframe: pd.DataFrame):\n",
    "    \"\"\"Preprocesses and tokenizes the given theses titles for further use.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    pattern = r\"\\b[\\wäöüÄÖÜß]+(?:[-'][\\wäöüÄÖÜß]+)*\\b\"\n",
    "    df_german = dataframe[dataframe.Sprache == \"DE\"]\n",
    "    tokens = [re.findall(pattern, str(row).lower(), flags=re.UNICODE) for row in df_german.Titel]\n",
    "    return tokens\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3acde311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_pairs(data, word2idx, window_size, num_negatives=5):\n",
    "    \"\"\"Creates skip-gram (word, context, label) pairs with negative sampling.\"\"\"\n",
    "    train_pairs = []\n",
    "    vocab = list(word2idx.values())\n",
    "\n",
    "    # Positive skip-gram pairs (label = 1)\n",
    "    for title in data:\n",
    "        for target_idx in range(len(title)):\n",
    "            target_word = title[target_idx]\n",
    "            target_id = word2idx[target_word]\n",
    "\n",
    "            start = max(0, target_idx - window_size)\n",
    "            end = min(len(title), target_idx + window_size + 1)\n",
    "\n",
    "            for context_idx in range(start, end):\n",
    "                if context_idx != target_idx:\n",
    "                    context_word = title[context_idx]\n",
    "                    context_id = word2idx[context_word]\n",
    "                    train_pairs.append((target_id, context_id, 1))\n",
    "\n",
    "                    # Negative sampling (label = 0)\n",
    "                    for _ in range(num_negatives):\n",
    "                        negative_id = np.random.choice(vocab)\n",
    "                        while negative_id == context_id:\n",
    "                            negative_id = np.random.choice(vocab)\n",
    "                        train_pairs.append((target_id, negative_id, 0))\n",
    "\n",
    "    return train_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "299d6af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(email, am), (email, pc-cd-rom-steuerung), (email, kostenÿberwachung), (email, referenzmodells), (email, technologievergleich), (email, on-premises), (email, beispiel), (email, technologiebewertung), (email, impact), (email, modellbasierte), (email, networks), (email, werkzeuggestÿtzte), (am, email), (am, webgl-basierten), (am, liquiditštsmanagements), (am, strukturierung), (am, mannschaften), (am, unternehmenssoftware), (am, beispiel), (am, marktforschung), (von, untersuchung), (von, cyber-threat-modells), (von, automotive-spice-prozessmodellen), (von, fallbeispiel), (von, kompatibles), (von, e-mail), (von, systemzustšnden), (von, performancetestergebnissen), (automotive-spice-prozessmodellen, bereitstellung), (automotive-spice-prozessmodellen, fax), (automotive-spice-prozessmodellen, bucketlist), (automotive-spice-prozessmodellen, tcp), (automotive-spice-prozessmodellen, ua), (automotive-spice-prozessmodellen, axapta), (automotive-spice-prozessmodellen, von), (automotive-spice-prozessmodellen, regressabwicklung), (automotive-spice-prozessmodellen, gefahrensituationen), (automotive-spice-prozessmodellen, einfrieren), (automotive-spice-prozessmodellen, wirtschaftsinformatik), (automotive-spice-prozessmodellen, verschiedene), "
     ]
    }
   ],
   "source": [
    "dataframe = load_theses_dataset(\"C:\\\\Users\\\\Felix Rittmaier\\\\PythonProjects\\\\seqlrn_assignments\\\\4-nnet\\\\data\\\\theses.tsv\")\n",
    "tokenized_data = preprocess(dataframe)\n",
    "vocabulary = set()\n",
    "for tokens in tokenized_data:\n",
    "    vocabulary.update(tokens)\n",
    "  \n",
    "word2idx = {\n",
    "    word: idx for idx, word in enumerate(vocabulary)\n",
    "}\n",
    "idx2word = {\n",
    "    idx: word for idx, word in enumerate(vocabulary)\n",
    "}\n",
    "\n",
    "training_pairs = create_training_pairs(tokenized_data, word2idx, 2)\n",
    "\n",
    "for target_idx, context_idx, label in training_pairs[:20]:\n",
    "    print(f\"({idx2word[target_idx]}, {idx2word[context_idx]})\", end=\", \")\n",
    "\n",
    "for target_idx, context_idx, label in training_pairs[-20:]:\n",
    "    print(f\"({idx2word[target_idx]}, {idx2word[context_idx]})\", end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8bb6",
   "metadata": {},
   "source": [
    "### Train and Analyze\n",
    "\n",
    "2.1 Implement and train the word2vec model with your training data.\n",
    "\n",
    "2.2 Implement a method to find the top-k similar words for a given word (token).\n",
    "\n",
    "2.3 Analyze: What are the most similar words to \"Konzeption\", \"Cloud\" and \"virtuelle\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a65b62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "Epoch 1, Loss: 5441.7821\n",
      "Epoch 2, Loss: 2865.9892\n"
     ]
    }
   ],
   "source": [
    "### TODO: 2.1 Implement and train the word2vec model.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "VOCAB_SIZE = len(vocabulary)\n",
    "EMBEDDING_SIZE = 300  # from the original word2vec paper\n",
    "EPOCHS = 50\n",
    "\n",
    "class SkipGramDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, training_pairs):\n",
    "        \"\"\"\n",
    "        training_pairs: list of tuples (target_id, context_id, label)\n",
    "        \"\"\"\n",
    "        self.pairs = training_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target, context, label = self.pairs[idx]\n",
    "        return (\n",
    "            torch.tensor(target, dtype=torch.long),\n",
    "            torch.tensor(context, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.float),\n",
    "        )\n",
    "\n",
    "class Word2Vec(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(vocab_size, embed_size)\n",
    "        self.linear2 = torch.nn.Linear(embed_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "dataset = SkipGramDataset(training_pairs)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "one_hot_encoding = np.zeros(shape=(len(vocabulary), len(vocabulary)))\n",
    "np.fill_diagonal(one_hot_encoding ,1)\n",
    "print(one_hot_encoding)\n",
    "\n",
    "model = Word2Vec(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    for target_idx, context_idx, label in dataloader:\n",
    "        one_hot_target = torch.tensor(one_hot_encoding[target_idx], dtype=torch.float)\n",
    "\n",
    "        output = model(one_hot_target)\n",
    "\n",
    "        # Predict only context word columns\n",
    "        pred = output.gather(1, context_idx.view(-1, 1))\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(pred.squeeze(), label)\n",
    "\n",
    "        # Backprop and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {e+1}, Loss: {total_loss:.4f}\")\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02549c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.2 Implement a method to find the top-k similar words.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "def similarity_search(word_idx, trained_weights, k=10):\n",
    "    word_embedding = trained_weights[word_idx]\n",
    "    sim_scores = []\n",
    "\n",
    "    for i, vec in enumerate(trained_weights):\n",
    "        if i == word_idx:\n",
    "            continue  # skip comparing the word to itself\n",
    "\n",
    "        norm_product = np.linalg.norm(word_embedding) * np.linalg.norm(vec)\n",
    "        if norm_product == 0:  # avoid division with 0\n",
    "            similarity = 0\n",
    "        else:\n",
    "            similarity = np.dot(word_embedding, vec) / norm_product\n",
    "\n",
    "        sim_scores.append((i, similarity))\n",
    "\n",
    "    sim_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sim_scores[:k]\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fa1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.3 Find the most similar words for \"Konzeption\", \"Cloud\" and \"virtuelle\".\n",
    "\n",
    "### YOUR CODE HERE\n",
    "print(model)\n",
    "trained_weights = model[0].weight\n",
    "\n",
    "print(similarity_search(word2idx[\"konzeption\"], trained_weights, k=5))\n",
    "print(similarity_search(word2idx[\"cloud\"], trained_weights, k=5))\n",
    "print(similarity_search(word2idx[\"virtuelle\"], trained_weights, k=5))\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe48a9b",
   "metadata": {},
   "source": [
    "### Play with the Embeddings\n",
    "\n",
    "3.1 Use the computed embeddings: Can you identify the most similar theses for some examples?\n",
    "\n",
    "3.2 Visualize the embeddings for a subset of theses using [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). You can use [Scikit-Learn](https://scikit-learn.org/stable/) and [Matplotlib](https://matplotlib.org) or [Seaborn](https://seaborn.pydata.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 3.1 Compute the embeddings for the theses and transform with TSNE.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a5251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 3.2 Visualize the samples in the 2D space.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
